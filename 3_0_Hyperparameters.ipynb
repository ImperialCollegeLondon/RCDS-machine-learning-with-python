{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Hyperparameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve performance of a particular model, we will need to tune its *hyperparameter values* - i.e. the parameters that are not learned from the data but specified independently. Cross-validation allows us to make a sweep of the possible hyperparameter space and find combinations of hyperparameters that work well for the training data as a whole.\n",
    "\n",
    "Once we have decided on the best values for hyperparameters, we can train a final model on the *entire* training dataset and evaluate on the testing data for an independent assessment of performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at another classification dataset. Here we are attempting to distinguish between nasal and oral vowel sounds, using the amplitudes of the first five harmonics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "phoneme = fetch_openml(name='phoneme', version=1, parser='auto')\n",
    "X, y = phoneme.data.to_numpy(), phoneme.target.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could try a KNN classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, random_state=10)\n",
    "\n",
    "# We will use a smaller training set to make the problem harder\n",
    "X_train_ = X_train[:100]\n",
    "y_train_ = y_train[:100]\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train_, y_train_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assess using the [F1 score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test, y_pred, pos_label='1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good - but could we do better with a different value of *k*?\n",
    "\n",
    "We can do an exhaustive search of the hyperparameter space using [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "parameters = {'n_neighbors':[1, 2, 5, 10, 20, 50]}\n",
    "predictor = KNeighborsClassifier()\n",
    "gs = GridSearchCV(predictor, \n",
    "                  parameters, \n",
    "                  cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=40),\n",
    "                  scoring=make_scorer(f1_score, pos_label='1')\n",
    "                  )\n",
    "gs.fit(X_train_,y_train_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us some detailed results for each of the 5 splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will also report the best parameter values found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.cv_results_['rank_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-parameter searches\n",
    "\n",
    "Let's try a more complex example on the same dataset. Support Vector Machines have several hyperparameters that could be varied - for example, in addition to the kernel function itself, we have a regularisation parameter `C` to tune (a positive real value).\n",
    "\n",
    "`GridSearchCV` makes it easy for us to explore the space of possible hyperparameter values and choose the best combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "parameters = {'kernel':('linear', 'rbf', 'poly'), 'C':[0.01, 0.1, 1, 10, 100]}\n",
    "predictor = SVC()\n",
    "gs = GridSearchCV(predictor, \n",
    "                  parameters, \n",
    "                  cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=40),\n",
    "                  scoring=make_scorer(f1_score, pos_label='1')\n",
    "                  )\n",
    "gs.fit(X_train_,y_train_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.cv_results_['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then to train the final model we could do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = SVC(**gs.best_params_)\n",
    "final.fit(X_train_,y_train_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = final.predict(X_test)\n",
    "f1_score(y_test, y_pred, pos_label='1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly disappointing? Notice that the cross-validation can still overestimate performance on onseen data - this is why it is important to have a final test dataset available to obtain a convincing assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GridSearchCV with a pipeline\n",
    "\n",
    "When we have preprocessing steps to consider, the process becomes a little more complex. Remember that we will have to learn the transformations from *each split* in the training data. The pipeline can help here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will go back to the *autoMpg* regression dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mpg = fetch_openml(name='autoMpg', version=1, parser='auto')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(mpg.data, mpg.target, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we will add a LASSO predictor to the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Defines preprocessing transformations for specified columns\n",
    "ct = ColumnTransformer([ ('encode', OneHotEncoder(), ['origin']),\n",
    "                         ('impute', IterativeImputer(), ['horsepower'])\n",
    "                       ],\n",
    "                       remainder='passthrough') \n",
    "\n",
    "# Defines individual steps in a workflow\n",
    "pipe = Pipeline([('preprocessing', ct),\n",
    "                 ('scaling', StandardScaler()),\n",
    "                 ('predict', Lasso())])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note how we link the hyperparameter to the specific pipeline step\n",
    "parameters = {'predict__alpha':[0.001,0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "gs = GridSearchCV(pipe, \n",
    "                  parameters, \n",
    "                  cv=5,\n",
    "                  scoring='r2'\n",
    "                  )\n",
    "gs.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing steps can also have hyperparameters\n",
    "\n",
    "The problem is currently fairly easy as there are only seven features to consider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a load of noisy random features to make things more difficult:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_features = X_train.shape\n",
    "random_state = np.random.RandomState(12)\n",
    "random_data = random_state.randn(n_samples, 300 * n_features)\n",
    "X = pd.concat([X_train.reset_index(drop=True), \n",
    "               pd.DataFrame(random_data)], \n",
    "               axis=1)\n",
    "X.columns = X.columns.astype(str)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.fit(X,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this situation, a dimensionality reduction step would help to reduce the noise. \n",
    "Let's include a PCA step in the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pipe2 = Pipeline([('preprocessing', ct),\n",
    "                  ('scaling', StandardScaler()),\n",
    "                  ('reduce', PCA()),\n",
    "                  ('predict', Lasso())])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of PCA components is now a hyperparameter, so let's include it in the grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note how we link the hyperparameter to the specific pipeline step\n",
    "parameters = {'predict__alpha':[0.001,0.01,0.1,1,10,100],\n",
    "              'reduce__n_components':[2,3,4,5,6,7,8,9]}\n",
    "\n",
    "gs = GridSearchCV(pipe2, \n",
    "                  parameters, \n",
    "                  cv=5,\n",
    "                  scoring='r2'\n",
    "                  )\n",
    "gs.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Multi-layer perceptrons are sensitive to feature scaling, so it is highly recommended to scale your data. Using a pipeline, investigate whether scaling affects performance of an [MLPRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#mlpregressor) on the `wine_quality_white` dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "w = fetch_openml(name='wine-quality-white',version=1,parser='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLPRegressor has a lot of tunable hyperparameters. \n",
    "\n",
    "* `hidden_layer_sizes`\n",
    "* `activation`\n",
    "* `solver`\n",
    "* `alpha`\n",
    "* `learning_rate`\n",
    "* ...\n",
    "\n",
    "Use GridSearchCV to try to optimise its performance on this dataset (choose a few of the parameters to explore).\n",
    "\n",
    "Note that when different solvers have different parameter options, we can provide `GridSearchCV` with a list of dictionaries instead of a single dictionary. See [this example](https://scikit-learn.org/stable/auto_examples/neural_networks/plot_mlp_training_curves.html#sphx-glr-auto-examples-neural-networks-plot-mlp-training-curves-py) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
